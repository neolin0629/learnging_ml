# 最小二乘法(OLS)

主要思想是通过确定参数矩阵，使得真实值和预测值的误差(也称残差)平方和(SSE)最小。在预测变量里加入非线性变换，线性模型便可以轻松地纳入不同类型的非线性关系。

$$
E=\sum_{i=1}^n e_i^2=\sum_{i=1}^n(y_i-\hat{y_i})^2
$$

这里我们假设函数 $f(x_i)$ 是线性的，即 $y_i=x_i'g+\xi_i$（可以通过在 $x_i$ 中加入预测变量的非线性变换来纳入非线性关系）。

## 求解

方法一：分别对参数求导，联立方程令偏导等于 0，然后通过高斯消元法解出。

方法二：矩阵求解。目标函数(SSE)对 $g$ 进行微分，令一阶导数为零并求解 $g$ ，可得估计量 $\hat{g}=(X'X)^{-1}X'y$

样本内的拟合值 $\hat{y}=X\hat{g}$

## 特点

当高维环境中，即协变量远大于样本数量时，

1. OLS 会调整 $\hat{g}$ 来拟合噪声，因而造成严重的过拟合。
2. OLS 估计量此时不唯一，存在无穷多个能完美拟合数据的 $\hat{g}$ ，但这种拟合很大程度上是错误的拟合了 $\xi_i$ 而非拟合真实的 $f(x_i)$

# 岭回归

当协变量数量很大时，通过惩罚估计量 $\hat{g}$ 来避免过拟合。优化的目标函数是在 SSE 的基础上，补充了 L2 范数罚项 $g'g$。

$$
\min_g[\frac{1}{N}(y-Xg)'(y-Xg)+\gamma g'g]
$$

在这个目标函数里，第一项代表 Loss，即 OLS 回归会最小化的部分。第二项代表罚项，其中超参数 $\gamma$ 控制惩罚的强度，求解可得 $\hat{g}=(X'X+\gamma I_k)^{-1}X'y$，其中 $I_k$ 是 K \* K 的单位矩阵。

罚项的存在将对 $\hat{g}$ 中取值幅度过高的元素进行惩罚，岭回归最终得到的回归系数估计值比 OLS 更接近于零（OLS 可以视作 $\gamma = 0$ 的特殊情况)，但不会等于零。

## 特点

相对于 OLS，以损失部分信息、降低精度为代价获得回归系数(因为罚项，不再是无偏估计)，对存在离群点的数据的拟合要强于最小二乘法。

同时因为估计量趋向于 0，可以缓解多重共线以及过拟合问题，但又因为没有将系数收缩到 0，某些时候模型的解释性会大大降低，也无法从根本上解决多重共线问题。

# Lasso

Lasso 回归将罚项由 L2 范数变为 L1 范数 $\lVert g \rVert_1=\sum_{j=1}^K \lvert g_j \rvert$，可以将一些不重要的回归系数缩减为 0，达到剔除变量的目的。

$$
\min_g[\frac{1}{N}(y-Xg)'(y-Xg)+\gamma \sum_{j=1}^K|g_j|]
$$

Lasso 的解不会线性依赖于 y 且不存在解析式(由于罚项是绝对值，导致了在零点处不可导)，但可以使用最小角回归(LARS)算法等求出数值解。同时，$\hat{g}$ 可以是稀疏的系数估计值，只包含少数个非零元素，相当于对协变量进行了自动选择。

## 特点

当变量之间相关时，Lasso 会遇到问题。高度正相关的两个协变量，参数的估计值在参数矩阵里会是什么样(存在一个，还是同时存在两个)，很可能取决于噪音。

为了提升模型性能，最好的办法是用岭回归的做法，在模型中同时使用两个协变量的均值。

# 弹性网(Elastic Net)

结合了岭回归和 Lasso 的罚项，与 Lasso 一样，弹性网会将一些回归系数降为 0，但它不像 Lasso 那样强调变量选择，而是会类似岭回归，使回归系数趋向于 0。

$$
\min_g[\frac{1}{N}(y-Xg)'(y-Xg)+\lambda \rho \sum_{j=1}^K|g_j|+\frac{\lambda(1-\rho)}{2} g'g]
$$

可以看到，当 $\rho=0$ 时，其代价函数就等同于岭回归的代价函数，当 $\rho=1$ 时，其代价函数就等同于 Lasso 回归的代价函数。
